{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# National Mental Health Datahton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from functools import reduce\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from io import BytesIO\n",
    "import difflib\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from contextlib import redirect_stdout\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from textblob import TextBlob\n",
    "from typing import Tuple, Dict, Any\n",
    "import pyspssio\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capture print outputs\n",
    "def capture_print(func, filename):\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        func()\n",
    "    output = f.getvalue()\n",
    "    if output.strip(): \n",
    "        with open(f'text_outputs/{filename}.txt', 'w') as file:\n",
    "            file.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Guarding Minds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Guarding Minds (gm_df) .SAV file\n",
    "gm_file_path = 'input/Mental Health Research Canada/Guarding Minds - Mental Health in the Workplace/Guarding minds 2023_weighted_15.6.2023 Final.sav'\n",
    "gm_df, gm_meta = pyspssio.read_sav(gm_file_path)\n",
    "\n",
    "print(gm_df.head())\n",
    "\n",
    "# Basic dataset information\n",
    "print(f\"Dataset shape: {gm_df.shape}\")\n",
    "print(f\"Number of rows: {gm_df.shape[0]}, Number of columns: {gm_df.shape[1]}\")\n",
    "\n",
    "# Display column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(gm_df.columns.tolist())\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nData types:\")\n",
    "print(gm_df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(gm_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explore and apply SPSS metadata\n",
    "def process_spss_metadata(df, meta):\n",
    "    print(f\"Dataset has {len(df.columns)} variables\")\n",
    "    \n",
    "    # 1. Create a data dictionary\n",
    "    var_info = []\n",
    "    for col in df.columns:\n",
    "        info = {\n",
    "            'variable': col,\n",
    "            'label': meta.get('var_labels', {}).get(col, ''),\n",
    "            'values': meta.get('var_value_labels', {}).get(col, {}),\n",
    "            'type': meta.get('var_types', {}).get(col, ''),\n",
    "            'measure': meta.get('var_measure_levels', {}).get(col, '')\n",
    "        }\n",
    "        var_info.append(info)\n",
    "    \n",
    "    data_dict = pd.DataFrame(var_info)\n",
    "    \n",
    "    # 2. Create versions of the DataFrame\n",
    "    # Original with variable attributes\n",
    "    for col in df.columns:\n",
    "        if col in meta.get('var_labels', {}):\n",
    "            df[col].attrs['label'] = meta['var_labels'][col]\n",
    "        if col in meta.get('var_value_labels', {}):\n",
    "            df[col].attrs['value_labels'] = meta['var_value_labels'][col]\n",
    "    \n",
    "    # Version with readable column names\n",
    "    labeled_columns_df = df.copy()\n",
    "    labeled_columns_df.columns = [meta.get('var_labels', {}).get(col, col) for col in df.columns]\n",
    "    \n",
    "    # Version with values replaced by labels - UPDATED APPROACH\n",
    "    labeled_values_df = df.copy()\n",
    "    for col, labels in meta.get('var_value_labels', {}).items():\n",
    "        if col in labeled_values_df.columns:\n",
    "            # Using apply with lambda instead of map+fillna\n",
    "            labeled_values_df[col] = labeled_values_df[col].apply(\n",
    "                lambda x: labels.get(x, x) if pd.notna(x) else x\n",
    "            )\n",
    "    \n",
    "    return {\n",
    "        'original': df,\n",
    "        'data_dictionary': data_dict,\n",
    "        'labeled_columns': labeled_columns_df,\n",
    "        'labeled_values': labeled_values_df\n",
    "    }\n",
    "\n",
    "# Apply the function to your data\n",
    "results = process_spss_metadata(gm_df, gm_meta)\n",
    "\n",
    "# Extract each component into separate variables for easier access\n",
    "gm_original_df = results['original']  # Original DataFrame with attributes\n",
    "gm_data_dictionary = results['data_dictionary']  # Data dictionary with variable info\n",
    "gm_labeled_columns_df = results['labeled_columns']  # DataFrame with human-readable column names\n",
    "gm_labeled_values_df = results['labeled_values']  # DataFrame with human-readable values\n",
    "\n",
    "# Display information about each component\n",
    "print(\"\\n=== Original DataFrame ===\")\n",
    "print(f\"Shape: {gm_original_df.shape}\")\n",
    "print(gm_original_df.head())\n",
    "\n",
    "print(\"\\n=== Data Dictionary ===\")\n",
    "print(f\"Number of variables: {len(gm_data_dictionary)}\")\n",
    "print(gm_data_dictionary.head())\n",
    "\n",
    "print(\"\\n=== DataFrame with Labeled Columns ===\")\n",
    "print(f\"Shape: {gm_labeled_columns_df.shape}\")\n",
    "print(gm_labeled_columns_df.head())\n",
    "\n",
    "print(\"\\n=== DataFrame with Labeled Values ===\")\n",
    "print(f\"Shape: {gm_labeled_values_df.shape}\")\n",
    "print(gm_labeled_values_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all columns and save column names along with their data types to csv\n",
    "def save_column_names_to_csv(df, df_type):\n",
    "    filename = f\"text_outputs/guarding_minds/{df_type}_survey_columns.csv\"\n",
    "    \n",
    "    # Create a DataFrame with the column names and their data types\n",
    "    column_info = pd.DataFrame({\n",
    "        'Column Names': df.columns,\n",
    "        'Data Type': [df.dtypes[col] for col in df.columns]\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    column_info.to_csv(filename, index=False)\n",
    "    print(f\"\\nColumn names and data types have been saved to {filename}\")\n",
    "    \n",
    "save_column_names_to_csv(gm_df, 'original')\n",
    "save_column_names_to_csv(gm_labeled_columns_df, 'labeled_columns')\n",
    "save_column_names_to_csv(gm_labeled_values_df, 'labeled_values')\n",
    "\n",
    "# Save your processed data for future use\n",
    "gm_original_df.to_pickle('processed_data/guarding_minds/original_df.pkl')\n",
    "gm_data_dictionary.to_csv('processed_data/guarding_minds/data_dictionary.csv', index=False)\n",
    "gm_labeled_columns_df.to_pickle('processed_data/guarding_minds/labeled_columns_df.pkl')\n",
    "gm_labeled_values_df.to_pickle('processed_data/guarding_minds/labeled_values_df.pkl')\n",
    "\n",
    "print(\"\\nAll processed datasets have been saved to the 'processed_data' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set specific guarding minds output path\n",
    "gm_output_path = 'plot_outputs/guarding_minds'\n",
    "\n",
    "# missing values heatmap\n",
    "def plot_missing_values_heatmap(df, output_path):\n",
    "    plt.figure(figsize=(40, 20))\n",
    "    sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')\n",
    "    plt.title('Missing Values Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_path}/missing_values_heatmap.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_missing_values_heatmap(gm_labeled_values_df, gm_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value percentages\n",
    "def plot_missing_values_bars(df, output_path):\n",
    "    plt.figure(figsize=(20, 36))\n",
    "    missing_percentages = (df.isnull().sum() / len(df) * 100).sort_values(ascending=True)\n",
    "    sns.barplot(x=missing_percentages.values, y=missing_percentages.index)\n",
    "    plt.title('Percentage of Missing Values by Column')\n",
    "    plt.xlabel('Percentage Missing')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_path}/missing_values_percentage.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_missing_values_bars(gm_labeled_values_df, gm_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with 'Missing' for every object column and convert to category type\n",
    "gm_labeled_values_df_columns = gm_labeled_values_df.columns\n",
    "\n",
    "for c in gm_labeled_values_df_columns:\n",
    "    if gm_labeled_values_df[c].dtype == \"object\":\n",
    "        # Step 1: Replace NaN values with 'Missing'\n",
    "        gm_labeled_values_df[c] = gm_labeled_values_df[c].fillna(\"Missing\")\n",
    "        \n",
    "        # Step 2: Convert the column from object type to category type\n",
    "        gm_labeled_values_df[c] = gm_labeled_values_df[c].astype('category')\n",
    "        \n",
    "        # Optional: Print conversion confirmation for debugging\n",
    "        print(f\"Column '{c}' converted to category with dtype: {gm_labeled_values_df[c].dtype}\")\n",
    "\n",
    "# Print summary of categorical features\n",
    "print(f\"In these features, there are {len(gm_labeled_values_df_columns)} CATEGORICAL FEATURES: {gm_labeled_values_df_columns}\")\n",
    "\n",
    "# Optional: Print memory usage improvement\n",
    "print(f\"Memory usage after conversion: {gm_labeled_values_df.memory_usage().sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot categorical distributions\n",
    "def plot_categorical_distributions(df, output_path):\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    for col in tqdm.tqdm(categorical_cols, desc=\"Creating categorical plots\"):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        value_counts = df[col].value_counts()\n",
    "        sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_path}/categorical_{col}.png')\n",
    "        plt.close()\n",
    "\n",
    "# labeled values data frame\n",
    "plot_categorical_distributions(gm_labeled_values_df, gm_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical distributions\n",
    "def plot_numerical_distributions(df, output_path):\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns\n",
    "    \n",
    "    # Create progress bar for numerical distributions\n",
    "    for col in tqdm.tqdm(numerical_cols, desc=\"Creating distribution plots\"):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create subplot with histogram and kde\n",
    "        sns.histplot(data=df, x=col, kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Add statistical annotations\n",
    "        stats_text = f'Mean: {df[col].mean():.2f}\\n'\n",
    "        stats_text += f'Median: {df[col].median():.2f}\\n'\n",
    "        stats_text += f'Std: {df[col].std():.2f}'\n",
    "        plt.text(0.95, 0.95, stats_text,\n",
    "                transform=plt.gca().transAxes,\n",
    "                verticalalignment='top',\n",
    "                horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_path}/distribution_{col}.png')\n",
    "        plt.close()\n",
    "\n",
    "# numerical distributions of labeled values\n",
    "plot_numerical_distributions(gm_labeled_values_df, gm_output_path)\n",
    "\n",
    "# numerical distributions of original\n",
    "plot_numerical_distributions(gm_df, gm_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each province\n",
    "gm_province_counts = gm_labeled_values_df['PROV'].value_counts().reset_index()\n",
    "gm_province_counts.columns = ['province_code', 'count']\n",
    "\n",
    "# Print the counts to verify\n",
    "print(\"Province counts:\")\n",
    "print(gm_province_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix Newfoundland label\n",
    "gm_province_counts['province_code'] = gm_province_counts['province_code'].astype(str)\n",
    "gm_province_counts.loc[gm_province_counts['province_code'] == 'Newfoundland', 'province_code'] = 'Newfoundland and Labrador'\n",
    "gm_province_counts['province_code'] = gm_province_counts['province_code'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get canada geojson file\n",
    "canada_geojson_url = \"https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/canada.geojson\"\n",
    "canada_geojson_response = requests.get(canada_geojson_url)\n",
    "canada_geojson = json.loads(canada_geojson_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create base map\n",
    "canada_map = folium.Map(location=[56.1304, -106.3468], zoom_start=3, \n",
    "                       tiles='CartoDB positron')\n",
    "\n",
    "# Add choropleth layer\n",
    "choropleth = folium.Choropleth(\n",
    "    geo_data=canada_geojson,\n",
    "    name='choropleth',\n",
    "    data=gm_province_counts,\n",
    "    columns=['province_code', 'count'],\n",
    "    key_on='feature.properties.name',\n",
    "    fill_color='YlOrRd',  # Yellow-Orange-Red color scheme\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.5,\n",
    "    legend_name='Number of occurrences in PROV column',\n",
    "    highlight=True\n",
    ").add_to(canada_map)\n",
    "\n",
    "# Add tooltips to the choropleth layer\n",
    "choropleth.geojson.add_child(\n",
    "    folium.features.GeoJsonTooltip(['name'], labels=False)\n",
    ")\n",
    "\n",
    "# Add a layer control\n",
    "folium.LayerControl().add_to(canada_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in gm_province_counts.iterrows():\n",
    "    # Find the matching province in the GeoJSON\n",
    "    for feature in canada_geojson['features']:\n",
    "        if feature['properties']['name'] == row['province_code']:\n",
    "            # Get approximate center of the province\n",
    "            geometry = feature['geometry']\n",
    "            if geometry['type'] == 'Polygon':\n",
    "                # For simple polygons\n",
    "                coords = np.array(geometry['coordinates'][0])\n",
    "                center = coords.mean(axis=0)\n",
    "                # Folium expects coordinates as [lat, lon]\n",
    "                center = [center[1], center[0]]\n",
    "            elif geometry['type'] == 'MultiPolygon':\n",
    "                # For complex shapes with multiple polygons\n",
    "                all_coords = []\n",
    "                for polygon in geometry['coordinates']:\n",
    "                    coords = np.array(polygon[0])\n",
    "                    all_coords.append(coords)\n",
    "                all_coords = np.vstack(all_coords)\n",
    "                center = all_coords.mean(axis=0)\n",
    "                center = [center[1], center[0]]\n",
    "            else:\n",
    "                # Skip if geometry type is unexpected\n",
    "                continue\n",
    "                \n",
    "            # Add a text label with the count\n",
    "            folium.Marker(\n",
    "                location=center,\n",
    "                icon=folium.DivIcon(\n",
    "                    icon_size=(50,36),\n",
    "                    icon_anchor=(12.5, 4),\n",
    "                    html=f'<div style=\"font-size: 12pt; font-weight: bold; text-align: center; background-color: rgba(255, 255, 255, 0.7); padding: 1px 1px; border-radius: 1px;\">{row[\"count\"]}</div>'\n",
    "                ),\n",
    "                popup=f\"{row['province_code']}: {row['count']} respondents\"\n",
    "            ).add_to(canada_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a simple legend in the corner\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; \n",
    "            bottom: 50px; left: 50px; width: 250px; height: auto; \n",
    "            border:2px solid grey; z-index:9999; font-size:14px;\n",
    "            background-color:white;\n",
    "            padding: 10px;\n",
    "            border-radius: 5px;\n",
    "           \">\n",
    "    <div style=\"font-weight: bold; margin-bottom: 5px;\">Province Count Legend</div>\n",
    "'''\n",
    "\n",
    "# Add entries for each province in descending order\n",
    "for _, row in gm_province_counts.sort_values('count', ascending=False).iterrows():\n",
    "    legend_html += f'<div>{row[\"province_code\"]}: {row[\"count\"]}</div>'\n",
    "\n",
    "legend_html += '</div>'\n",
    "\n",
    "# Add the legend as an HTML element\n",
    "canada_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Save the map to an HTML file\n",
    "canada_map.save('plot_outputs/guarding_minds/canada_province_heatmap.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canada_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Distress Centre Calgary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load Excel file with progress tracking\n",
    "def load_excel_with_progress(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        try:           \n",
    "            # Read the Excel file\n",
    "            print(f\"Reading excel file: {file_path}\")\n",
    "            df = pd.read_excel(file_path)\n",
    "            print(f\"Successfully loaded: {file_path}\")\n",
    "            print(f\"Shape: {df.shape} (rows, columns)\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Load the Distress Centre Calgary Data\n",
    "print(\"\\n====== LOADING DISTRESS CENTRE CALGARY DATA ======\\n\")\n",
    "\n",
    "# Define all file paths\n",
    "file_paths = {\n",
    "    'dccd_23_24_df': 'input/Distress Centre Calgary/Crisis and CT Raw data 2023-2024-cleanB.xlsx',\n",
    "    'dccd_21_22_df': 'input/Distress Centre Calgary/Crisis and CT Raw data 2021-2022-cleanB.xlsx',\n",
    "    'dccd_20_df': 'input/Distress Centre Calgary/Crisis and CT Raw data 2020-cleanB.xlsx',\n",
    "    'dccd_19_df': 'input/Distress Centre Calgary/Crisis and CT Raw data 2019 Old Call Form-clean.xlsx',\n",
    "    'dccd_17_18_df': 'input/Distress Centre Calgary/Crisis and CT Raw data 2017-2018-clean.xlsx',\n",
    "    'dccd_15_16_df': 'input/Distress Centre Calgary/Crisis and CT Raw data 2015-2016 - clean.xlsx'\n",
    "}\n",
    "\n",
    "# Create empty dictionary to store dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Load each file with progress bar\n",
    "for df_name, file_path in file_paths.items():\n",
    "    dataframes[df_name] = load_excel_with_progress(file_path)\n",
    "    print()  # Add a blank line between files for readability\n",
    "\n",
    "# Assign dataframes to their specific variables\n",
    "dccd_23_24_df = dataframes['dccd_23_24_df']\n",
    "dccd_21_22_df = dataframes['dccd_21_22_df']\n",
    "dccd_20_df = dataframes['dccd_20_df']\n",
    "dccd_19_df = dataframes['dccd_19_df']\n",
    "dccd_17_18_df = dataframes['dccd_17_18_df']\n",
    "dccd_15_16_df = dataframes['dccd_15_16_df']\n",
    "\n",
    "# Print overall loading summary\n",
    "print(\"\\n====== LOADING SUMMARY ======\")\n",
    "successful_loads = sum(1 for df in dataframes.values() if df is not None)\n",
    "print(f\"Successfully loaded {successful_loads} out of {len(file_paths)} files\")\n",
    "\n",
    "# Display a summary of each dataframe if loaded successfully\n",
    "print(\"\\n====== DATAFRAMES OVERVIEW ======\")\n",
    "for name, df in dataframes.items():\n",
    "    if df is not None:\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Rows: {df.shape[0]}\")\n",
    "        print(f\"  Columns: {df.shape[1]}\")\n",
    "        print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the data together - outer join\n",
    "def standardize_columns(df, df_name):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Add source information to track which dataset each row came from\n",
    "    df['source_dataset'] = df_name\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Standardize each dataframe\n",
    "standardized_dfs = []\n",
    "for name, df in [('dccd_23_24', dccd_23_24_df), \n",
    "                 ('dccd_21_22', dccd_21_22_df),\n",
    "                 ('dccd_20', dccd_20_df),\n",
    "                 ('dccd_19', dccd_19_df),\n",
    "                 ('dccd_17_18', dccd_17_18_df),\n",
    "                 ('dccd_15_16', dccd_15_16_df)]:\n",
    "    standardized_dfs.append(standardize_columns(df, name))\n",
    "    \n",
    "def append_with_all_columns():\n",
    "    # Concatenate all dataframes\n",
    "    result_df = pd.concat(standardized_dfs, ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "dccd_all_df = append_with_all_columns()\n",
    "\n",
    "print(f\"Result with all columns shape: {dccd_all_df.shape}\")\n",
    "\n",
    "print(\"\\nPreview of result with all columns:\")\n",
    "display(dccd_all_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean column names\n",
    "def clean_column_names(df):\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Replace special characters with underscores using regular expressions\n",
    "    # This keeps letters, numbers, and underscores, replacing everything else\n",
    "    df_clean.columns = [re.sub(r'[^\\w]', '_', col) for col in df_clean.columns]\n",
    "    \n",
    "    # Remove consecutive underscores (e.g., '__' becomes '_')\n",
    "    df_clean.columns = [re.sub(r'_+', '_', col) for col in df_clean.columns]\n",
    "    \n",
    "    # Remove leading/trailing underscores\n",
    "    df_clean.columns = [col.strip('_') for col in df_clean.columns]\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply the cleaning function to our dataframe\n",
    "dccd_all_df = clean_column_names(dccd_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the consolidated data\n",
    "dccd_all_df.to_csv('processed_data/distress_centre_calgary/original_df.csv', index=False)\n",
    "dccd_all_df.to_pickle('processed_data/distress_centre_calgary/original_df.pkl')\n",
    "\n",
    "# save column names\n",
    "filename = f\"text_outputs/distress_centre_calgary/consolidated_df_columns.csv\"\n",
    "    \n",
    "# Create a DataFrame with the column names and their data types\n",
    "dccd_column_info = pd.DataFrame({\n",
    "    'Column Names': dccd_all_df.columns,\n",
    "    'Data Type': [dccd_all_df.dtypes[col] for col in dccd_all_df.columns]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "dccd_column_info.to_csv(filename, index=False)\n",
    "print(f\"\\nColumn names and data types have been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set specific guarding minds output path\n",
    "dccd_output_path = 'plot_outputs/distress_centre_calgary'\n",
    "\n",
    "plot_missing_values_heatmap(dccd_all_df, dccd_output_path)\n",
    "plot_missing_values_bars(dccd_all_df, dccd_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add removal of empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with 'Missing' for every object column and convert to category type\n",
    "dccd_all_df_columns = dccd_all_df.columns\n",
    "\n",
    "for c in dccd_all_df_columns:\n",
    "    if dccd_all_df[c].dtype == \"object\":\n",
    "        # Replace NaN values with 'Missing'\n",
    "        dccd_all_df[c] = dccd_all_df[c].fillna(\"Missing\")\n",
    "        \n",
    "        # Convert the column from object type to category type\n",
    "        dccd_all_df[c] = dccd_all_df[c].astype('category')\n",
    "        \n",
    "        # Print conversion confirmation for debugging\n",
    "        print(f\"Column '{c}' converted to category with dtype: {dccd_all_df[c].dtype}\")\n",
    "\n",
    "# Print summary of categorical features\n",
    "print(f\"In these features, there are {len(dccd_all_df_columns)} CATEGORICAL FEATURES: {dccd_all_df_columns}\")\n",
    "\n",
    "# Optional: Print memory usage improvement\n",
    "print(f\"Memory usage after conversion: {dccd_all_df.memory_usage().sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot categorical distributions\n",
    "plot_categorical_distributions(dccd_all_df, dccd_output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
